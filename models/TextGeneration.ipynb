{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextGeneration.ipynb","provenance":[{"file_id":"https://github.com/google/trax/blob/master/trax/models/reformer/text_generation.ipynb","timestamp":1603779061849}],"private_outputs":true,"collapsed_sections":["udDs_biH0n5U"],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"udDs_biH0n5U"},"source":["#### Copyright 2020 Google LLC."]},{"cell_type":"markdown","metadata":{"id":"psnUF-8c02o_"},"source":["# Reformer: Text Generation [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"1lnRd_IoERdk"},"source":["This notebook was designed to run on TPU.\n","\n","To use TPUs in Colab, click \"Runtime\" on the main menu bar and select Change runtime type. Set \"TPU\" as the hardware accelerator."]},{"cell_type":"code","metadata":{"id":"8PluCmWbZIpJ"},"source":["# Install JAX.\n","!pip install --upgrade jax\n","!pip install --upgrade jaxlib\n","!pip install --upgrade trax\n","\n","# Make sure the Colab Runtime is set to Accelerator: TPU.\n","import requests\n","import os\n","if 'TPU_DRIVER_MODE' not in globals():\n","  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n","  resp = requests.post(url)\n","  TPU_DRIVER_MODE = 1\n","\n","# The following is required to use TPU Driver as JAX's backend.\n","from jax.config import config\n","config.FLAGS.jax_xla_backend = \"tpu_driver\"\n","config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n","print(config.FLAGS.jax_backend_target)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yiPdBenoZwH6"},"source":["!pip install --upgrade -q sentencepiece\n","!pip install --upgrade -q gin \n","!pip install bert_score\n","\n","from tensorflow.compat.v1.io.gfile import GFile\n","import gin\n","import os\n","import jax\n","import trax\n","from trax.data import inputs\n","import pickle\n","\n","import numpy as np\n","import jax.numpy as jnp\n","from bert_score import score\n","\n","from scipy.special import softmax\n","\n","from sentencepiece import SentencePieceProcessor"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GcgguYpumZ4x"},"source":["# ユーザー関数\n","def write_text(text, id):\n","    with open(\"sample.txt\", \"a\", encoding=\"utf-8\") as f:\n","        f.write(\"start_id: {}, text: {}\".format(str(id), text))\n","        f.write(\"\\n\")\n","        \n","def calc_bert_score(cands, refs):\n","    \"\"\" BERTスコアの算出\n","\n","    Args:\n","        cands ([List[str]]): [比較元の文]\n","        refs ([List[str]]): [比較対象の文]\n","\n","    Returns:\n","        [(List[float], List[float], List[float])]: [(Precision, Recall, F1スコア)]\n","    \"\"\"\n","    Precision, Recall, F1 = score(cands, refs, lang=\"ja\", verbose=True)\n","    return Precision.numpy().tolist(), Recall.numpy().tolist(), F1.numpy().tolist()\n","\n","def loadPickle(fileName):\n","    with open(fileName, mode=\"rb\") as f:\n","        return pickle.load(f)\n","\n","def dumpPickle(fileName, obj):\n","    with open(fileName, mode=\"wb\") as f:\n","        pickle.dump(obj, f)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQ89jHCYfhpg"},"source":["## Setting up data and model"]},{"cell_type":"markdown","metadata":{"id":"9_OCIqghSyfs"},"source":["In this notebook, we'll be pushing the limits of just how many tokens we can fit on a single TPU device. The TPUs available in Colab have 8GB of memory per core, and 8 cores. We will set up a Reformer model that can fit a copy of \"Crime and Punishment\" on *each* of the 8 TPU cores (over 500,000 tokens per 8GB of memory)."]},{"cell_type":"code","metadata":{"id":"tYSOVGR47LVL"},"source":["# corpusデータを読み込み\n","corpus = loadPickle(\"corpus.pickle\")\n","# 長文順にソート\n","# corpus.sort(key=len, reverse=True)\n","print(len(corpus))\n","\n","text = \"\".join(corpus[5000:10000])\n","print(len(text))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMntV3H-6OR0"},"source":["# Load a BPE vocabulaary with 320 types. This mostly consists of single letters\n","# and pairs of letters, but it has some common words and word pieces, too.\n","# !gsutil cp gs://trax-ml/reformer/cp.320.* .\n","\n","TOKENIZER = SentencePieceProcessor()\n","TOKENIZER.load('./utils/shogi.model')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HnJzxSi_77zP"},"source":["# Tokenize\n","IDS = TOKENIZER.EncodeAsIds(text)\n","IDS = np.asarray(IDS, dtype=np.int32)\n","PAD_AMOUNT = 256 * 512 - len(IDS)\n","print(\"Number of tokens:\", IDS.shape[0])\n","print(\"Number of PAD AMOUNT:\", PAD_AMOUNT)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bzQ7G9uGSga5"},"source":["As we see above, \"Crime and Punishment\" has just over half a million tokens with the BPE vocabulary we have selected.\n","\n","Normally we would have a dataset with many examples, but for this demonstration we fit a language model on the single novel only. We don't want the model to just memorize the dataset by encoding the words in its position embeddings, so at each training iteration we will randomly select how much padding to put before the text vs. after it.\n","\n","We have 8 TPU cores, so we will separately randomize the amount of padding for each core."]},{"cell_type":"code","metadata":{"id":"PdAwmpS220ub"},"source":["# Set up the data pipeline.\n","def my_inputs(n_devices):\n","  while True:\n","    inputs = []\n","    mask = []\n","    pad_amounts = np.random.choice(PAD_AMOUNT, n_devices)\n","    for i in range(n_devices):\n","      inputs.append(np.pad(IDS, (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n","                            mode='constant'))\n","      mask.append(np.pad(np.ones_like(IDS, dtype=np.float32),\n","                          (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n","                          mode='constant'))\n","    inputs = np.stack(inputs)\n","    mask = np.stack(mask)\n","    yield (inputs, inputs, mask)\n","\n","print(\"(device count, tokens per device) = \",\n","      next(my_inputs(trax.fastmath.device_count()))[0].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ei90LdK024r_"},"source":["# Configure hyperparameters.\n","gin.parse_config(\"\"\"\n","import trax.layers\n","import trax.models\n","import trax.optimizers\n","import trax.data.inputs\n","import trax.supervised.trainer_lib\n","\n","# Parameters that will vary between experiments:\n","# ==============================================================================\n","train.model = @trax.models.ReformerLM\n","# Our model will have 6 layers, alternating between the LSH attention proposed\n","# in the Reformer paper and local attention within a certain context window.\n","n_layers = 6\n","attn_type = [\n","  @trax.layers.SelfAttention,\n","  @LSHSelfAttention,  \n","  @trax.layers.SelfAttention,\n","  @LSHSelfAttention,\n","  @trax.layers.SelfAttention,\n","  @LSHSelfAttention,\n","  ]\n","share_qk = False  # LSH attention ignores this flag and always shares q & k\n","n_heads = 2\n","attn_kv = 128\n","dropout = 0.10\n","n_tokens = 131072\n","\n","# Parameters for multifactor:\n","# ==============================================================================\n","multifactor.constant = 0.01\n","multifactor.factors = 'constant * linear_warmup * cosine_decay'\n","multifactor.warmup_steps = 100\n","multifactor.steps_per_cycle = 900\n","\n","# Parameters for Adam:\n","# Parameters for RMSProp:\n","# ==============================================================================\n","Adam.weight_decay_rate=0.0\n","Adam.b1 = 0.86\n","Adam.b2 = 0.92\n","Adam.eps = 1e-9\n","\n","# Parameters for RMSProp: 試行してみたものの、数100epochでlossが下がらず断念。\n","# ==============================================================================\n","# learning_rate=0.001\n","# gamma=0.9\n","# eps=1e-08\n","# clip_grad_norm=None\n","\n","# Parameters for SelfAttention:\n","# ==============================================================================\n","trax.layers.SelfAttention.attention_dropout = 0.10\n","trax.layers.SelfAttention.chunk_len = 64\n","trax.layers.SelfAttention.n_chunks_before = 1\n","trax.layers.SelfAttention.n_parallel_heads = 1\n","\n","# Parameters for LSHSelfAttention:\n","# ==============================================================================\n","LSHSelfAttention.attention_dropout = 0.2\n","LSHSelfAttention.chunk_len = 64\n","LSHSelfAttention.n_buckets = [64, 128]\n","LSHSelfAttention.n_chunks_after = 0\n","LSHSelfAttention.n_chunks_before = 1\n","LSHSelfAttention.n_hashes = 1\n","LSHSelfAttention.n_parallel_heads = 1\n","LSHSelfAttention.predict_drop_len = 128\n","LSHSelfAttention.predict_mem_len = 1024\n","\n","# Parameters for ReformerLM:\n","# ==============================================================================\n","ReformerLM.attention_type = %attn_type\n","ReformerLM.d_attention_key = %attn_kv\n","ReformerLM.d_attention_value = %attn_kv\n","ReformerLM.d_model = 256\n","ReformerLM.d_ff = 512\n","ReformerLM.dropout = %dropout\n","ReformerLM.ff_activation = @trax.layers.Relu\n","ReformerLM.max_len = %n_tokens\n","ReformerLM.mode = 'train'\n","ReformerLM.n_heads = %n_heads\n","ReformerLM.n_layers = %n_layers\n","ReformerLM.vocab_size = 6000\n","ReformerLM.axial_pos_shape = (256, 512)\n","ReformerLM.d_axial_pos_embs= (64, 192)\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kw8WW0Ou1tlA"},"source":["## 学習時のみ使用"]},{"cell_type":"code","metadata":{"id":"RGGt0WaT3a-h"},"source":["# Set up a Trainer.\n","output_dir = os.path.expanduser('~/train_dir/')\n","# output_dir = os.getcwd()\n","!rm -f ~/train_dir/model.pkl.gz  # Remove old model\n","\n","trainer = trax.supervised.Trainer(\n","    model=trax.models.ReformerLM,\n","    loss_fn=trax.layers.CrossEntropyLoss(),\n","    optimizer=trax.optimizers.Adam,\n","    lr_schedule=trax.lr.multifactor(),\n","    inputs=trax.data.inputs.Inputs(my_inputs),\n","    output_dir=output_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y6VQkmKO3a1L"},"source":["# Run one training step, to make sure the model fits in memory.\n","# The first time trainer.train_epoch is called, it will JIT the entire network\n","# architecture, which takes around 2 minutes. The JIT-compiled model is saved\n","# so subsequent runs will be much faster than the first.\n","trainer.train_epoch(n_steps=1, n_eval_steps=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFnX4G6z3asD"},"source":["# Train for 600 steps total\n","# The first ~20 steps are slow to run, but after that it reaches steady-state\n","# speed. This will take at least 30 minutes to run to completion, but can safely\n","# be interrupted by selecting \"Runtime > Interrupt Execution\" from the menu.\n","# The language model won't be exceptionally good when trained for just a few\n","# steps and with minimal regularization. However, we can still sample from it to\n","# see what it learns.\n","trainer.train_epoch(n_steps=9, n_eval_steps=1)\n","for _ in range(300):\n","  trainer.train_epoch(n_steps=10, n_eval_steps=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B2fCOGMmcHOv"},"source":["cp /root/train_dir/model.pkl.gz ./model.pkl.gz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zY3hpgnI5Rgn"},"source":["## 文章生成時に使用"]},{"cell_type":"code","metadata":{"id":"ffeLSbJk35pv"},"source":["# As we report in the Reformer paper, increasing the number of hashing rounds\n","# helps with quality. We can even increase the number of hashing rounds at\n","# evaluation time only.\n","\n","gin.parse_config(\"\"\"LSHSelfAttention.n_hashes = 4\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BwIjdl6_2tX"},"source":["# Load the trained Reformer in 'predict' mode\n","model = trax.models.ReformerLM(mode='predict')\n","# model.init_from_file(os.path.join(os.getcwd(),'model.pkl.gz'),\n","#                      weights_only=True)\n","\n","model.init_from_file(os.path.join(os.getcwd(),'model/model.pkl.gz'),\n","                     weights_only=True)\n","# Sample from ReformerLM\n","output_token_ids = trax.supervised.decoding.autoregressive_sample(\n","    model, temperature=0.0)\n","\n","# Decode token IDs\n","# Reformer outputed a batch with one item, we access it using [0]\n","# tolist() converts from int64 to int, the type SentencePiece expects\n","TOKENIZER = SentencePieceProcessor()\n","TOKENIZER.load('./utils/shogi.model')\n","\n","TOKENIZER.DecodeIds(output_token_ids[0].tolist()) \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1T006lPhiWwN"},"source":["# 文章生成\n","sample_corpus = []\n","ids = np.random.randint(4, 6000, 300)\n","\n","for i, id in enumerate(ids):\n","    print(\"-------------------processing{}----------------------\".format(str(i)))\n","    # 引数のstart_idかtemperatureを変更することでランダム生成できる\n","    out = trax.supervised.decoding.autoregressive_sample(model, temperature=0.0, start_id=id, max_length=30)\n","    sample_text = TOKENIZER.DecodeIds(out[0].tolist())\n","    sample_corpus.append(sample_text)\n","\n","## 生成文書を保存したい場合\n","## pickle形式\n","# DumpPickle(sample_corpus, \"sample_corpus\")"],"execution_count":null,"outputs":[]}]}